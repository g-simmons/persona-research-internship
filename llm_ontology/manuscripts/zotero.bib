@article{kuhn_landscape_2024,
	title = {A landscape of consciousness: Toward a taxonomy of explanations and implications},
	volume = {190},
	issn = {0079-6107},
	url = {https://www.sciencedirect.com/science/article/pii/S0079610723001128},
	doi = {10.1016/j.pbiomolbio.2023.12.003},
	shorttitle = {A landscape of consciousness},
	abstract = {Diverse explanations or theories of consciousness are arrayed on a roughly physicalist-to-nonphysicalist landscape of essences and mechanisms. Categories: Materialism Theories (philosophical, neurobiological, electromagnetic field, computational and informational, homeostatic and affective, embodied and enactive, relational, representational, language, phylogenetic evolution); Non-Reductive Physicalism; Quantum Theories; Integrated Information Theory; Panpsychisms; Monisms; Dualisms; Idealisms; Anomalous and Altered States Theories; Challenge Theories. There are many subcategories, especially for Materialism Theories. Each explanation is self-described by its adherents, critique is minimal and only for clarification, and there is no attempt to adjudicate among theories. The implications of consciousness explanations or theories are assessed with respect to four questions: meaning/purpose/value (if any); {AI} consciousness; virtual immortality; and survival beyond death. A Landscape of Consciousness, I suggest, offers perspective.},
	pages = {28--169},
	journaltitle = {Progress in Biophysics and Molecular Biology},
	shortjournal = {Progress in Biophysics and Molecular Biology},
	author = {Kuhn, Robert Lawrence},
	urldate = {2024-08-05},
	date = {2024-08-01},
	keywords = {Consciousness, Dualism, Idealism, Materialism, Mind-body problem, Monism},
}

@misc{gholizadeh_novel_2020,
	title = {A Novel Method of Extracting Topological Features from Word Embeddings},
	url = {http://arxiv.org/abs/2003.13074},
	doi = {10.48550/arXiv.2003.13074},
	abstract = {In recent years, topological data analysis has been utilized for a wide range of problems to deal with high dimensional noisy data. While text representations are often high dimensional and noisy, there are only a few work on the application of topological data analysis in natural language processing. In this paper, we introduce a novel algorithm to extract topological features from word embedding representation of text that can be used for text classification. Working on word embeddings, topological data analysis can interpret the embedding high-dimensional space and discover the relations among different embedding dimensions. We will use persistent homology, the most commonly tool from topological data analysis, for our experiment. Examining our topological algorithm on long textual documents, we will show our defined topological features may outperform conventional text mining features.},
	number = {{arXiv}:2003.13074},
	publisher = {{arXiv}},
	author = {Gholizadeh, Shafie and Seyeditabari, Armin and Zadrozny, Wlodek},
	urldate = {2024-07-24},
	date = {2020-04-19},
	eprinttype = {arxiv},
	eprint = {2003.13074 [cs, math, stat]},
	keywords = {68U15, 68T50, Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7, Mathematics - Algebraic Topology, Statistics - Machine Learning},
}

@article{frey_benchmarking_nodate,
	title = {Benchmarking the Abilities of Large Language Models for {RDF} Knowledge Graph Creation and Comprehension: How Well Do {LLMs} Speak Turtle?},
	abstract = {Large Language Models ({LLMs}) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various {LLMs}, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the {LLM}-{KG}-Bench. The evaluation encompassed four commercially available {LLMs} - {GPT}-3.5, {GPT}-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, {GPT}4All Vicuna and {GPT}4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of {LLMs} in relation to their application within {RDF} knowledge graph engineering workflows utilizing Turtle representation. While our findings show that the latest commercial models outperform their forerunners in terms of proficiency with the Turtle language, they also reveal an apparent weakness. These models fall short when it comes to adhering strictly to the output formatting constraints, a crucial requirement in this context.},
	author = {Frey, Johannes and Meyer, Lars-Peter and Arndt, Natanael and Brei, Felix and Bulert, Kirill},
	langid = {english},
}

@misc{mai_llms_2024,
	title = {Do {LLMs} Really Adapt to Domains? An Ontology Learning Perspective},
	url = {http://arxiv.org/abs/2407.19998},
	doi = {10.48550/arXiv.2407.19998},
	shorttitle = {Do {LLMs} Really Adapt to Domains?},
	abstract = {Large Language Models ({LLMs}) have demonstrated unprecedented prowess across various natural language processing tasks in various application domains. Recent studies show that {LLMs} can be leveraged to perform lexical semantic tasks, such as Knowledge Base Completion ({KBC}) or Ontology Learning ({OL}). However, it has not effectively been verified whether their success is due to their ability to reason over unstructured or semi-structured data, or their effective learning of linguistic patterns and senses alone. This unresolved question is particularly crucial when dealing with domain-specific data, where the lexical senses and their meaning can completely differ from what a {LLM} has learned during its training stage. This paper investigates the following question: Do {LLMs} really adapt to domains and remain consistent in the extraction of structured knowledge, or do they only learn lexical senses instead of reasoning? To answer this question and, we devise a controlled experiment setup that uses {WordNet} to synthesize parallel corpora, with English and gibberish terms. We examine the differences in the outputs of {LLMs} for each corpus in two {OL} tasks: relation extraction and taxonomy discovery. Empirical results show that, while adapting to the gibberish corpora, off-the-shelf {LLMs} do not consistently reason over semantic relationships between concepts, and instead leverage senses and their frame. However, fine-tuning improves the performance of {LLMs} on lexical semantic tasks even when the domain-specific terms are arbitrary and unseen during pre-training, hinting at the applicability of pre-trained {LLMs} for {OL}.},
	number = {{arXiv}:2407.19998},
	publisher = {{arXiv}},
	author = {Mai, Huu Tan and Chu, Cuong Xuan and Paulheim, Heiko},
	urldate = {2024-08-03},
	date = {2024-07-29},
	eprinttype = {arxiv},
	eprint = {2407.19998 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{michel_does_2017,
	location = {Vancouver, Canada},
	title = {Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology-Based Representations},
	url = {https://aclanthology.org/W17-2628},
	doi = {10.18653/v1/W17-2628},
	shorttitle = {Does the Geometry of Word Embeddings Help Document Classification?},
	abstract = {We investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space. In this work, we evaluate the utility of these topology-based document representations in traditional {NLP} tasks, specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact, performance is worse than simple techniques like tf-idf, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.},
	eventtitle = {{RepL}4NLP 2017},
	pages = {235--240},
	booktitle = {Proceedings of the 2nd Workshop on Representation Learning for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Michel, Paul and Ravichander, Abhilasha and Rijhwani, Shruti},
	editor = {Blunsom, Phil and Bordes, Antoine and Cho, Kyunghyun and Cohen, Shay and Dyer, Chris and Grefenstette, Edward and Hermann, Karl Moritz and Rimell, Laura and Weston, Jason and Yih, Scott},
	urldate = {2024-07-24},
	date = {2017-08},
}

@article{procko_gpt-4_2023,
	title = {{GPT}-4: A Stochastic Parrot or Ontological Craftsman? Discovering Implicit Knowledge Structures in Large Language Models},
	rights = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10387600/},
	doi = {10.1109/TransAI60598.2023.00043},
	shorttitle = {{GPT}-4},
	abstract = {Ontologies are representational artifacts that purport to accurately portray the aspect of reality under the purview of the ontologists laboring upon them. Ontologies exist in a spectrum of formality, from lexical thesauri to knowledge graphs, to collections of statements of first-order logic. The recent proliferation of Large Language Models ({LLMs}) has brought to bear interactive “knowledge bases” with general awareness of most things. As ontologists create ontologies from their understanding of reality; and as {LLMs}, presumably, possess some “understanding” of reality, embedded in their vector matrices corresponding to lexical terms from massive quantities of learned texts, a question is posed: what form of ontology can an {LLM} create when prompted about some novel facet of reality, without explicitly asking it for an ontology? I.e., will an {LLM} categorize things into bins, or a subsumption hierarchy, or perhaps something else? {LLMs}, as they are understood, respond when prompted with the most likely response, because they are predictors of next tokens, i.e., they are stochastic parrots. In any case, it is posited that, if prompted without any explicit request for an ontology, an {LLM} can produce an ontology of novel form, effectively granting insight into the “understanding” an {LLM} has of the world, as all humans possess an understanding of the world that ontologies are based upon. This paper explores the use of the flagship {LLM}, {GPT}-4, in forming an ontology of a novel domain.},
	pages = {147--154},
	journaltitle = {2023 Fifth International Conference on Transdisciplinary {AI} ({TransAI})},
	author = {Procko, Tyler Thomas and Elvira, Timothy and Ochoa, Omar},
	urldate = {2024-08-03},
	date = {2023-09-25},
	note = {Conference Name: 2023 Fifth International Conference on Transdisciplinary {AI} ({TransAI})
{ISBN}: 9798350358018
Place: Laguna Hills, {CA}, {USA}
Publisher: {IEEE}},
}

@misc{fitz_hidden_2024,
	title = {Hidden Holes: topological aspects of language models},
	url = {http://arxiv.org/abs/2406.05798},
	shorttitle = {Hidden Holes},
	abstract = {We explore the topology of representation manifolds arising in autoregressive neural language models trained on raw text data. In order to study their properties, we introduce tools from computational algebraic topology, which we use as a basis for a measure of topological complexity, that we call perforation.},
	number = {{arXiv}:2406.05798},
	publisher = {{arXiv}},
	author = {Fitz, Stephen and Romero, Peter and Schneider, Jiyan Jonas},
	urldate = {2024-07-28},
	date = {2024-06-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.05798 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{herbold_large_2024,
	title = {Large Language Models can impersonate politicians and other public figures},
	url = {https://www.semanticscholar.org/paper/Large-Language-Models-can-impersonate-politicians-Herbold-Trautsch/8de94a446cb48012c763693fc5bf5e42ab7ec4ad},
	abstract = {Modern {AI} technology like Large language models ({LLMs}) has the potential to pollute the public information sphere with made-up content, which poses a significant threat to the cohesion of societies at large. A wide range of research has shown that {LLMs} are capable of generating text of impressive quality, including persuasive political speech, text with a pre-defined style, and role-specific content. But there is a crucial gap in the literature: We lack large-scale and systematic studies of how capable {LLMs} are in impersonating political and societal representatives and how the general public judges these impersonations in terms of authenticity, relevance and coherence. We present the results of a study based on a cross-section of British society that shows that {LLMs} are able to generate responses to debate questions that were part of a broadcast political debate programme in the {UK}. The impersonated responses are judged to be more authentic and relevant than the original responses given by people who were impersonated. This shows two things: (1) {LLMs} can be made to contribute meaningfully to the public political debate and (2) there is a dire need to inform the general public of the potential harm this can have on society.},
	author = {Herbold, Steffen and Trautsch, Alexander and Kikteva, Zlata and Hautli-Janisz, Annette},
	urldate = {2024-08-05},
	date = {2024-07-09},
}

@article{giglou_llms4ol_2023,
	title = {{LLMs}4OL: Large Language Models for Ontology Learning},
	rights = {Creative Commons Attribution Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2307.16648},
	doi = {10.48550/ARXIV.2307.16648},
	shorttitle = {{LLMs}4OL},
	abstract = {We propose the {LLMs}4OL approach, which utilizes Large Language Models ({LLMs}) for Ontology Learning ({OL}). {LLMs} have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our {LLMs}4OL paradigm investigates the following hypothesis: {\textbackslash}textit\{Can {LLMs} effectively apply their language pattern capturing capability to {OL}, which involves automatically extracting and structuring knowledge from natural language text?\} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different {LLM} model families for three main {OL} tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in {WordNet}, geographical knowledge in {GeoNames}, and medical knowledge in {UMLS}.},
	author = {Giglou, Hamed Babaei and D'Souza, Jennifer and Auer, Sören},
	urldate = {2024-08-03},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Information Theory (cs.{IT}), Machine Learning (cs.{LG})},
}

@misc{groeneveld_olmo_2024,
	title = {{OLMo}: Accelerating the Science of Language Models},
	url = {http://arxiv.org/abs/2402.00838},
	doi = {10.48550/arXiv.2402.00838},
	shorttitle = {{OLMo}},
	abstract = {Language models ({LMs}) have become ubiquitous in both {NLP} research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open {LMs}. To this end, we have built {OLMo}, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release {OLMo} alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.},
	number = {{arXiv}:2402.00838},
	publisher = {{arXiv}},
	author = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi Raghavi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
	urldate = {2024-08-03},
	date = {2024-06-07},
	eprinttype = {arxiv},
	eprint = {2402.00838 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{khadir_ontology_2021,
	title = {Ontology learning: Grand tour and challenges},
	volume = {39},
	issn = {1574-0137},
	url = {https://www.sciencedirect.com/science/article/pii/S1574013720304391},
	doi = {10.1016/j.cosrev.2020.100339},
	shorttitle = {Ontology learning},
	abstract = {Ontologies are at the core of the semantic web. As knowledge bases, they are very useful resources for many artificial intelligence applications. Ontology learning, as a research area, proposes techniques to automate several tasks of the ontology construction process to simplify the tedious work of manually building ontologies. In this paper we present the state of the art of this field. Different classes of approaches are covered (linguistic, statistical, and machine learning), including some recent ones (deep-learning-based approaches). In addition, some relevant solutions (frameworks), which offer strategies and built-in methods for ontology learning, are presented. A descriptive summary is made to point out the capabilities of the different contributions based on criteria that have to do with the produced ontology components and the degree of automation. We also highlight the challenge of evaluating ontologies to make them reliable, since it is not a trivial task in this field; it actually represents a research area on its own. Finally, we identify some unresolved issues and open questions.},
	pages = {100339},
	journaltitle = {Computer Science Review},
	shortjournal = {Computer Science Review},
	author = {Khadir, Ahlem Chérifa and Aliane, Hassina and Guessoum, Ahmed},
	urldate = {2024-08-03},
	date = {2021-02-01},
	keywords = {Deep learning, Linguistic and statistical approaches, Machine learning, Ontologies, Ontology learning},
}

@misc{biderman_pythia_2023,
	title = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
	url = {http://arxiv.org/abs/2304.01373},
	doi = {10.48550/arXiv.2304.01373},
	shorttitle = {Pythia},
	abstract = {How do large language models ({LLMs}) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce {\textbackslash}textit\{Pythia\}, a suite of 16 {LLMs} all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend {\textbackslash}textit\{Pythia\} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward {LLMs} and their training dynamics. Trained models, analysis code, training code, and training data can be found at {\textbackslash}url\{https://github.com/{EleutherAI}/pythia\}.},
	number = {{arXiv}:2304.01373},
	publisher = {{arXiv}},
	author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, {USVSN} Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and van der Wal, Oskar},
	urldate = {2024-07-10},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2304.01373 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{caufield_structured_2024,
	title = {Structured Prompt Interrogation and Recursive Extraction of Semantics ({SPIRES}): a method for populating knowledge bases using zero-shot learning},
	volume = {40},
	issn = {1367-4811},
	url = {https://doi.org/10.1093/bioinformatics/btae104},
	doi = {10.1093/bioinformatics/btae104},
	shorttitle = {Structured Prompt Interrogation and Recursive Extraction of Semantics ({SPIRES})},
	abstract = {Creating knowledge bases and ontologies is a time consuming task that relies on manual curation. {AI}/{NLP} approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrarily complex nested knowledge schemas.Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics ({SPIRES}), a Knowledge Extraction approach that relies on the ability of Large Language Models ({LLMs}) to perform zero-shot learning and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, {SPIRES} recursively performs prompt interrogation against an {LLM} to obtain a set of responses matching the provided schema. {SPIRES} uses existing ontologies and vocabularies to provide identifiers for matched elements. We present examples of applying {SPIRES} in different domains, including extraction of food recipes, multi-species cellular signaling pathways, disease treatments, multi-step drug mechanisms, and chemical to disease relationships. Current {SPIRES} accuracy is comparable to the mid-range of existing Relation Extraction methods, but greatly surpasses an {LLM}’s native capability of grounding entities with unique identifiers. {SPIRES} has the advantage of easy customization, flexibility, and, crucially, the ability to perform new tasks in the absence of any new training data. This method supports a general strategy of leveraging the language interpreting capabilities of {LLMs} to assemble knowledge bases, assisting manual knowledge curation and acquisition while supporting validation with publicly-available databases and ontologies external to the {LLM}.{SPIRES} is available as part of the open source {OntoGPT} package: https://github.com/monarch-initiative/ontogpt.},
	pages = {btae104},
	number = {3},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Caufield, J Harry and Hegde, Harshad and Emonet, Vincent and Harris, Nomi L and Joachimiak, Marcin P and Matentzoglu, Nicolas and Kim, {HyeongSik} and Moxon, Sierra and Reese, Justin T and Haendel, Melissa A and Robinson, Peter N and Mungall, Christopher J},
	urldate = {2024-08-03},
	date = {2024-03-01},
}

@inproceedings{mihindukulasooriya_text2kgbench_2023,
	title = {Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2308.02357},
	doi = {10.48550/ARXIV.2308.02357},
	shorttitle = {Text2KGBench},
	abstract = {The recent advances in large language models ({LLM}) and foundation models with emergent capabilities have been shown to improve the performance of many {NLP} tasks. {LLMs} and Knowledge Graphs ({KG}) can complement each other such that {LLMs} can be used for {KG} construction or completion while existing {KGs} can be used for different tasks such as making {LLM} outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate {KGs} from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-{TekGen} with 10 ontologies and 13,474 sentences and (ii) {DBpedia}-{WebNLG} with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact extraction performance, ontology conformance, and hallucinations by {LLMs}. Furthermore, we provide results for two baseline models, Vicuna-13B and Alpaca-{LoRA}-13B using automatic prompt generation from test cases. The baseline results show that there is room for improvement using both Semantic Web and Natural Language Processing techniques.},
	publisher = {{arXiv}},
	author = {Mihindukulasooriya, Nandana and Tiwari, Sanju and Enguix, Carlos F. and Lata, Kusum},
	urldate = {2024-08-03},
	date = {2023},
	note = {Version Number: 1},
	keywords = {68, Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, I.2.4; I.2.7},
}

@misc{mihindukulasooriya_text2kgbench_2023-1,
	title = {Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text},
	url = {http://arxiv.org/abs/2308.02357},
	doi = {10.48550/arXiv.2308.02357},
	shorttitle = {Text2KGBench},
	abstract = {The recent advances in large language models ({LLM}) and foundation models with emergent capabilities have been shown to improve the performance of many {NLP} tasks. {LLMs} and Knowledge Graphs ({KG}) can complement each other such that {LLMs} can be used for {KG} construction or completion while existing {KGs} can be used for different tasks such as making {LLM} outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate {KGs} from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-{TekGen} with 10 ontologies and 13,474 sentences and (ii) {DBpedia}-{WebNLG} with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact extraction performance, ontology conformance, and hallucinations by {LLMs}. Furthermore, we provide results for two baseline models, Vicuna-13B and Alpaca-{LoRA}-13B using automatic prompt generation from test cases. The baseline results show that there is room for improvement using both Semantic Web and Natural Language Processing techniques.},
	number = {{arXiv}:2308.02357},
	publisher = {{arXiv}},
	author = {Mihindukulasooriya, Nandana and Tiwari, Sanju and Enguix, Carlos F. and Lata, Kusum},
	urldate = {2024-08-03},
	date = {2023-08-04},
	eprinttype = {arxiv},
	eprint = {2308.02357 [cs]},
	keywords = {68, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.4, I.2.7},
}

@misc{park_geometry_2024,
	title = {The Geometry of Categorical and Hierarchical Concepts in Large Language Models},
	url = {http://arxiv.org/abs/2406.01506},
	doi = {10.48550/arXiv.2406.01506},
	abstract = {Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability. In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as \{'mammal', 'bird', 'reptile', 'fish'\}, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that 'dog' is a kind of 'mammal' encoded? We show how to extend the linear representation hypothesis to answer these questions. We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure. We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from {WordNet}.},
	number = {{arXiv}:2406.01506},
	publisher = {{arXiv}},
	author = {Park, Kiho and Choe, Yo Joong and Jiang, Yibo and Veitch, Victor},
	urldate = {2024-07-02},
	date = {2024-06-03},
	eprinttype = {arxiv},
	eprint = {2406.01506 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{park_linear_2024,
	title = {The Linear Representation Hypothesis and the Geometry of Large Language Models},
	url = {http://arxiv.org/abs/2311.03658},
	doi = {10.48550/arXiv.2311.03658},
	abstract = {Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with {LLaMA}-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.},
	number = {{arXiv}:2311.03658},
	publisher = {{arXiv}},
	author = {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
	urldate = {2024-07-31},
	date = {2024-07-17},
	eprinttype = {arxiv},
	eprint = {2311.03658 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{draganov_shape_2024,
	title = {The Shape of Word Embeddings: Recognizing Language Phylogenies through Topological Data Analysis},
	url = {http://arxiv.org/abs/2404.00500},
	doi = {10.48550/arXiv.2404.00500},
	shorttitle = {The Shape of Word Embeddings},
	abstract = {Word embeddings represent language vocabularies as clouds of \$d\$-dimensional points. We investigate how information is conveyed by the general shape of these clouds, outside of representing the semantic meaning of each token. Specifically, we use the notion of persistent homology from topological data analysis ({TDA}) to measure the distances between language pairs from the shape of their unlabeled embeddings. We use these distance matrices to construct language phylogenetic trees over 81 Indo-European languages. Careful evaluation shows that our reconstructed trees exhibit strong similarities to the reference tree.},
	number = {{arXiv}:2404.00500},
	publisher = {{arXiv}},
	author = {Draganov, Ondřej and Skiena, Steven},
	urldate = {2024-07-24},
	date = {2024-03-30},
	eprinttype = {arxiv},
	eprint = {2404.00500 [cs, math]},
	keywords = {Computer Science - Computation and Language, Mathematics - Algebraic Topology},
}
